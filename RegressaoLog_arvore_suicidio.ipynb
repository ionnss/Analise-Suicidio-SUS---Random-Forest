{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6527c9f-e84e-4440-8f39-0e27fb488c82",
   "metadata": {},
   "source": [
    "# Regressão logística\n",
    "\n",
    "## Aplicação Regressão Logística\n",
    "\n",
    "Para aplicar a regressão logística com pesos num quadro de dados, normalmente segue estes passos:\n",
    "\n",
    "1. Preparar os dados: Certifique-se de que o dataframe esteja limpo e pré-processado. Isso inclui o tratamento de valores ausentes, a codificação de variáveis categóricas e a normalização/padronização dos dados, se necessário.\n",
    "\n",
    "2. Atribuir pesos: Adicione uma coluna ao seu dataframe que contenha os pesos para cada amostra. Essa coluna será usada para aplicar pesos durante a regressão logística.\n",
    "\n",
    "3. Escolha uma implementação de regressão logística: Use uma biblioteca como scikit-learn, statsmodels ou outra biblioteca adequada que suporte regressão logística ponderada\n",
    "\n",
    "> Você pode calcular pesos de amostra baseado em alguma lógica que faça sentido para o seu problema. Um método comum é usar o inverso da frequência das classes para balancear o dataset, especialmente se você tiver um problema de desbalanceamento de classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f360dce-c922-4e4b-aff3-7e015b4725f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importar bibliotecas\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.utils.class_weight import compute_sample_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6389186c-8717-48a5-9b7b-0c2a9f66de0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7374431847198816\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.74      0.85    155159\n",
      "           1       0.03      0.74      0.05      1489\n",
      "\n",
      "    accuracy                           0.74    156648\n",
      "   macro avg       0.51      0.74      0.45    156648\n",
      "weighted avg       0.99      0.74      0.84    156648\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Carregar dataframe\n",
    "dataframe = pd.read_csv('df_atualizada_2.csv')\n",
    "\n",
    "# Preparar as variáveis independentes e dependentes\n",
    "dff = dataframe.drop('CAUSABAS', axis=1)\n",
    "X = dff.drop('suicidio', axis=1)\n",
    "y = dff['suicidio']\n",
    "\n",
    "# Dividir o conjunto de dados em treinamento e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Calcular pesos das amostras com base na distribuição das classes\n",
    "sample_weights = compute_sample_weight(class_weight='balanced', y=y_train)\n",
    "\n",
    "# Treinar o modelo de regressão logística com pesos\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train, sample_weight=sample_weights)\n",
    "\n",
    "# Fazer previsões com o conjunto de teste\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Avaliar o modelo\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c473e84e-ce68-4676-819d-63da1d5d8022",
   "metadata": {},
   "source": [
    "## Iterações\n",
    "\n",
    "O aviso de convergência que você está vendo sugere que o otimizador não conseguiu convergir para uma solução dentro do número padrão de iterações. Existem algumas abordagens que você pode tomar para resolver esse problema:\n",
    "\n",
    "1. Aumentar o número de iterações (max_iter):\n",
    "Você pode aumentar o número de iterações para permitir que o otimizador tenha mais tempo para convergir.\n",
    "\n",
    "2. Escalar os dados:\n",
    "Escalar os dados pode ajudar o otimizador a convergir mais rapidamente. Isso é especialmente importante para algoritmos de aprendizado de máquina baseados em gradiente, como a regressão logística.\n",
    "\n",
    "3. Experimentar diferentes solvers:\n",
    "O scikit-learn oferece vários solvers que você pode experimentar. Alguns solvers podem convergir melhor para certos tipos de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "306de7c9-540e-4645-bf7c-7a98b712b8e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.13975920535212707\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.13      0.23    155159\n",
      "           1       0.01      0.97      0.02      1489\n",
      "\n",
      "    accuracy                           0.14    156648\n",
      "   macro avg       0.50      0.55      0.13    156648\n",
      "weighted avg       0.99      0.14      0.23    156648\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#importar bibliotecas\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Escalar os dados\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Treinar o modelo de regressão logística com pesos\n",
    "model = LogisticRegression(class_weight='balanced', max_iter=1000, solver='saga')\n",
    "model.fit(X_train, y_train, sample_weight=sample_weights)\n",
    "\n",
    "# Fazer previsões com o conjunto de teste\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Avaliar o modelo\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5970c4-8265-4c10-bc41-dea6bc762817",
   "metadata": {},
   "source": [
    "## Aplicando SMOTE\n",
    "\n",
    "`SMOTE` *(Synthetic Minority Over-sampling Technique)* é uma técnica de oversampling usada para tratar problemas de desbalanceamento de classes em conjuntos de dados. Em problemas de classificação, quando uma classe é significativamente menos representada em comparação com outras (classe minoritária), o modelo pode ter dificuldade em aprender os padrões dessa classe, resultando em desempenho insatisfatório.\n",
    "\n",
    "### Como Funciona o SMOTE:\n",
    "\n",
    "1. Seleção dos Vizinhos:\n",
    "\n",
    "Para cada exemplo da classe minoritária, o SMOTE seleciona seus k vizinhos mais próximos usando uma métrica de distância, como a distância Euclidiana.\n",
    "\n",
    "\n",
    "2. Geração de Exemplos Sintéticos:\n",
    "\n",
    "Novos exemplos sintéticos são gerados ao longo das linhas que conectam os exemplos minoritários selecionados aos seus vizinhos.\n",
    "\n",
    "A diferença entre o exemplo minoritário original e o seu vizinho mais próximo é multiplicada por um número aleatório entre 0 e 1, e essa diferença é adicionada ao exemplo original, criando um novo ponto de dados.\n",
    "\n",
    "\n",
    "### Vantagens do SMOTE:\n",
    "\n",
    "1. Aumento da Variabilidade:\n",
    "\n",
    "Ao criar exemplos sintéticos, o SMOTE aumenta a variabilidade da classe minoritária, ajudando o modelo a generalizar melhor.\n",
    "\n",
    "2. Prevenção de Overfitting:\n",
    "\n",
    "Diferente do simples duplicação de exemplos minoritários, o SMOTE gera novos exemplos únicos, ajudando a prevenir o overfitting.\n",
    "\n",
    "### Desvantagens do SMOTE:\n",
    "\n",
    "1. Introdução de Ruído:\n",
    "\n",
    "Exemplos sintéticos podem estar em regiões do espaço de características onde a classe minoritária não deveria estar presente, introduzindo ruído.\n",
    "\n",
    "\n",
    "2. Computacionalmente Intenso:\n",
    "\n",
    "Para grandes conjuntos de dados, a geração de exemplos sintéticos pode ser computacionalmente intensiva.\n",
    "\n",
    "3. Implementação do SMOTE em Python:\n",
    "\n",
    "A biblioteca imblearn (Imbalanced-learn) oferece uma implementação conveniente do SMOTE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e025e976-38a7-4c81-a17d-fd2808004b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7646442980440222\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.76      0.87    155159\n",
      "           1       0.03      0.78      0.06      1489\n",
      "\n",
      "    accuracy                           0.76    156648\n",
      "   macro avg       0.51      0.77      0.46    156648\n",
      "weighted avg       0.99      0.76      0.86    156648\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Carregar dataframe\n",
    "dataframe = pd.read_csv('df_atualizada_2.csv')\n",
    "\n",
    "# Preparar as variáveis independentes e dependentes\n",
    "dff = dataframe.drop('CAUSABAS', axis=1)\n",
    "X = dff.drop('suicidio', axis=1)\n",
    "y = dff['suicidio']\n",
    "\n",
    "# Dividir o conjunto de dados em treinamento e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Escalar os dados\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Aplicar SMOTE para oversampling\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Treinar o modelo de regressão logística com os dados balanceados\n",
    "model = LogisticRegression(max_iter=1000, solver='saga')\n",
    "model.fit(X_train_res, y_train_res)\n",
    "\n",
    "# Fazer previsões com o conjunto de teste\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Avaliar o modelo\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99eafa9d-5ddf-4d04-b4dc-c1a11240ef84",
   "metadata": {},
   "source": [
    "### Observação\n",
    "\n",
    "Os resultados indicam que, apesar de ter utilizado SMOTE, o modelo ainda possui uma precisão muito baixa para a classe minoritária (1), enquanto o recall é razoavelmente alto. Isso sugere que o modelo está detectando a maioria dos casos da classe minoritária, mas também classificando muitos casos da classe majoritária como minoritária, o que resulta em baixa precisão.\n",
    "\n",
    "Aqui estão algumas sugestões adicionais para melhorar o desempenho:\n",
    "\n",
    "1. Ajuste Fino de Hiperparâmetros:\n",
    "\n",
    "`Você pode ajustar os hiperparâmetros do modelo para encontrar um melhor balanço entre precisão e recall.`\n",
    "\n",
    "2. Métodos Ensemble:\n",
    "\n",
    "`Random Forests e Gradient Boosting geralmente têm um melhor desempenho em problemas desbalanceados.`\n",
    "\n",
    "3. Experimentar Outros Solvers e Modelos:\n",
    "\n",
    "`Tentar outros modelos de regressão logística ou solvers, como liblinear, newton-cg, ou lbfgs.`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13f8b8d-456b-41c1-a223-40ede5c07002",
   "metadata": {},
   "source": [
    "### Ajustes de Hiperparâmetros\n",
    "\n",
    "No código abaixo, estamos: \n",
    "\n",
    "1. Aplicando `SMOTE` nos dados de treinamento`\n",
    "\n",
    "2. Utilizando a grade de ``Hiperparâmetros``, como os parâmetros ``Solve``, ``Iterações`` e ``Pesos de classes``, a fim de validação\n",
    "\n",
    "3. E realizando uma `Validação Cruzada` de modelos de `Regressão logística`, buscando identificar quais os melhores `PARÂMETROS`, da grade de Hiperparâmetros, e modelos de regressão\n",
    "\n",
    "4. Treinando o modelo com os melhores `PARÂMETROS` encontrados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2c19a97-4cff-41b6-b892-7fb0534dd690",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'class_weight': {0: 1, 1: 100}, 'max_iter': 1000, 'solver': 'lbfgs'}\n",
      "Best score: 0.9997841995517156\n",
      "Accuracy: 0.030131249680813033\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.02      0.04    155159\n",
      "           1       0.01      1.00      0.02      1489\n",
      "\n",
      "    accuracy                           0.03    156648\n",
      "   macro avg       0.50      0.51      0.03    156648\n",
      "weighted avg       0.99      0.03      0.04    156648\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Carregar o dataframe\n",
    "dataframe = pd.read_csv('df_atualizada_2.csv')\n",
    "\n",
    "# Preparar as variáveis independentes e dependentes\n",
    "dff = dataframe.drop('CAUSABAS', axis=1)\n",
    "X = dff.drop('suicidio', axis=1)\n",
    "y = dff['suicidio']\n",
    "\n",
    "# Dividir o conjunto de dados em treinamento e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Escalar os dados\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Aplicar SMOTE para oversampling\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Definir o grid de hiperparâmetros\n",
    "param_grid = {\n",
    "    'solver': ['lbfgs', 'liblinear', 'saga'],\n",
    "    'max_iter': [1000, 3000, 5000],\n",
    "    'class_weight': ['balanced', {0: 1, 1: 50}, {0: 1, 1: 100}]\n",
    "}\n",
    "\n",
    "# Realizar a busca em grade com validação cruzada\n",
    "grid = GridSearchCV(LogisticRegression(), param_grid, cv=5, scoring='recall')\n",
    "grid.fit(X_train_res, y_train_res)\n",
    "\n",
    "# Imprimir os melhores parâmetros encontrados\n",
    "print(\"Best parameters:\", grid.best_params_)\n",
    "print(\"Best score:\", grid.best_score_)\n",
    "\n",
    "# Treinar o modelo com os melhores parâmetros encontrados\n",
    "best_model = grid.best_estimator_\n",
    "best_model.fit(X_train_res, y_train_res)\n",
    "\n",
    "# Fazer previsões com o conjunto de teste\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Avaliar o modelo\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35918e9d-0e07-4778-9bbe-304d231f0691",
   "metadata": {},
   "source": [
    "### Manejo de Classes Desbalanceadas\n",
    "\n",
    "Problemas desbalanceados referem-se a situações em que uma classe (ou várias) na sua base de dados de treinamento têm muito menos exemplos do que outras. Isso é comum em problemas como detecção de fraudes, diagnósticos médicos raros, entre outros. Métodos como Random Forests e Gradient Boosting tendem a lidar melhor com isso porque:\n",
    "\n",
    "`Random Forests`: Por construir múltiplas árvores de decisão e combinar seus resultados, ele pode gerar modelos robustos que mitigam o impacto do desbalanceamento ao considerar diferentes subconjuntos dos dados.\n",
    "\n",
    "`Gradient Boosting`: Ao treinar iterativamente em cima dos erros dos modelos anteriores, Gradient Boosting foca nos exemplos mais difíceis de classificar, o que pode melhorar o desempenho nas classes minoritárias.\n",
    "\n",
    "- Embora esses métodos sejam eficazes, ajustes de parâmetros específicos (como pesos de classe ou taxas de aprendizado) ainda podem ser necessários para otimizar o desempenho em problemas desbalanceados. Isso pode incluir técnicas como oversampling (aumentar exemplos da classe minoritária) ou undersampling (reduzir exemplos da classe majoritária).\n",
    "\n",
    "- Enquanto Random Forests e Gradient Boosting são frequentemente recomendados, outros métodos como SVMs com kernels adequados ou redes neurais também podem ser eficazes dependendo do contexto e do conjunto de dados específico.\n",
    "\n",
    "\n",
    "#### Random Forest\n",
    "\n",
    "**Random Forest** é um algoritmo de aprendizado de máquina baseado em árvores de decisão. Ele cria uma \"floresta\" de árvores de decisão durante o treinamento e faz previsões agregando as previsões das árvores individuais. Esse método ajuda a reduzir o overfitting e melhora a precisão do modelo.\n",
    "\n",
    "##### Como funciona:\n",
    "\n",
    "1. **Ensemble de Árvores de Decisão**: Cria várias árvores de decisão, cada uma treinada com um subconjunto diferente dos dados.\n",
    "2. **Bagging (Bootstrap Aggregating)**: Utiliza amostragem com reposição para criar subconjuntos de dados para cada árvore.\n",
    "3. **Agregação**: A previsão final é feita agregando (por exemplo, votando ou tirando a média) as previsões de todas as árvores.\n",
    "\n",
    "##### Implementação:\n",
    "\n",
    "Vamos usar a biblioteca `scikit-learn` para treinar um modelo Random Forest. Utilizaremos a técnica SMOTE para balancear as classes.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Carregar o dataframe\n",
    "dataframe = pd.read_csv('df_atualizada_2.csv')\n",
    "\n",
    "# Preparar as variáveis independentes e dependentes\n",
    "dff = dataframe.drop('CAUSABAS', axis=1)\n",
    "X = dff.drop('suicidio', axis=1)\n",
    "y = dff['suicidio']\n",
    "\n",
    "# Dividir o conjunto de dados em treinamento e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Escalar os dados\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Aplicar SMOTE para oversampling\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Treinar o modelo Random Forest com pesos balanceados\n",
    "rf_model = RandomForestClassifier(class_weight='balanced', random_state=42)\n",
    "rf_model.fit(X_train_res, y_train_res)\n",
    "\n",
    "# Fazer previsões com o conjunto de teste\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Avaliar o modelo\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "```\n",
    "\n",
    "#### Gradient Boosting\n",
    "\n",
    "**Gradient Boosting** é um algoritmo de aprendizado de máquina que cria uma série de árvores de decisão, onde cada árvore corrige os erros do modelo anterior. É especialmente eficaz em conjuntos de dados desbalanceados porque pode se concentrar nos exemplos mal classificados.\n",
    "\n",
    "##### Como funciona:\n",
    "\n",
    "1. **Inicialização**: Começa com uma árvore de decisão simples.\n",
    "2. **Correção de Erros**: Cada árvore subsequente é treinada para corrigir os erros residuais das árvores anteriores.\n",
    "3. **Combinação de Árvores**: As árvores são combinadas para fazer a previsão final, geralmente através de uma média ponderada.\n",
    "\n",
    "##### Implementação:\n",
    "\n",
    "Vamos usar a biblioteca `scikit-learn` para treinar um modelo Gradient Boosting. Utilizaremos a técnica SMOTE para balancear as classes.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Carregar o dataframe\n",
    "dataframe = pd.read_csv('df_atualizada_2.csv')\n",
    "\n",
    "# Preparar as variáveis independentes e dependentes\n",
    "dff = dataframe.drop('CAUSABAS', axis=1)\n",
    "X = dff.drop('suicidio', axis=1)\n",
    "y = dff['suicidio']\n",
    "\n",
    "# Dividir o conjunto de dados em treinamento e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Escalar os dados\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Aplicar SMOTE para oversampling\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Treinar o modelo Gradient Boosting\n",
    "gb_model = GradientBoostingClassifier(random_state=42)\n",
    "gb_model.fit(X_train_res, y_train_res)\n",
    "\n",
    "# Fazer previsões com o conjunto de teste\n",
    "y_pred = gb_model.predict(X_test)\n",
    "\n",
    "# Avaliar o modelo\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "```\n",
    "\n",
    "#### Explicação das Etapas:\n",
    "\n",
    "1. **Carregar e Preparar os Dados**:\n",
    "   - Carregamos os dados e preparamos as variáveis independentes (X) e dependentes (y).\n",
    "2. **Divisão do Conjunto de Dados**:\n",
    "   - Dividimos os dados em conjuntos de treinamento e teste.\n",
    "3. **Escalar os Dados**:\n",
    "   - Normalizamos os dados para garantir que todas as características estejam na mesma escala.\n",
    "4. **Aplicar SMOTE**:\n",
    "   - Usamos o SMOTE para balancear as classes no conjunto de treinamento.\n",
    "5. **Treinar o Modelo**:\n",
    "   - Treinamos o modelo Random Forest ou Gradient Boosting com os dados balanceados.\n",
    "6. **Avaliar o Modelo**:\n",
    "   - Avaliamos o desempenho do modelo usando métricas como acurácia, precisão, recall e f1-score.\n",
    "\n",
    "Essas abordagens podem ajudar a melhorar o desempenho do seu modelo em conjuntos de dados desbalanceados. Tente executar esses códigos e observe as mudanças nas métricas de avaliação."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a299ed-22a9-4ead-b188-fdf12f6f176b",
   "metadata": {},
   "source": [
    "### Aplicando Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aeb6f9d1-6fa9-4c13-9f22-4dd066464f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8325353659159389\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.83      0.91    155159\n",
      "           1       0.04      0.72      0.08      1489\n",
      "\n",
      "    accuracy                           0.83    156648\n",
      "   macro avg       0.52      0.78      0.49    156648\n",
      "weighted avg       0.99      0.83      0.90    156648\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Carregar o dataframe\n",
    "dataframe = pd.read_csv('df_atualizada_2.csv')\n",
    "\n",
    "# Preparar as variáveis independentes e dependentes\n",
    "dff = dataframe.drop('CAUSABAS', axis=1)\n",
    "X = dff.drop('suicidio', axis=1)\n",
    "y = dff['suicidio']\n",
    "\n",
    "# Dividir o conjunto de dados em treinamento e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Escalar os dados\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Aplicar SMOTE para oversampling\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Treinar o modelo Gradient Boosting\n",
    "gb_model = GradientBoostingClassifier(random_state=42)\n",
    "gb_model.fit(X_train_res, y_train_res)\n",
    "\n",
    "# Fazer previsões com o conjunto de teste\n",
    "y_pred = gb_model.predict(X_test)\n",
    "\n",
    "# Avaliar o modelo\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a8e3b8-7f08-403c-a8e9-ea4b59b598bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf124e9-1243-4087-8a71-33b1cc5eca08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddd0103f-423d-491a-922a-de67432329cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n# definindo variáveis para as variáveis dependentes e independentes (target e data)\\nfrom sklearn import tree\\n\\n# carregando dataframe para modelo de previsao\\ndataframe = pd.read_csv('df_atualizada_2.csv')\\n\\ndff = dataframe.drop('CAUSABAS', axis=1)\\n\\n# definindo as variaveis dependente (y) e independentes (X)\\nX = dff.drop('suicidio', axis=1) # novo dataframe de variáveis independentes e retirar variável dependente ('suicidio' -> axis = eixo das colunas)\\ny = dff['suicidio']\\n\\n# Dividir o conjunto de dados em treinamento e teste\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\n\\n# treinar classificador\\nclf = tree.DecisionTreeClassifier(random_state=42)\\nclf = clf.fit(X_train, y_train)\\n\\n# Fazer previsões com o conjunto de teste\\ny_pred = clf.predict(X_test)\\n\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316e0d3d-2caf-4993-b556-f07edb87458f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
